{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c371ad32",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b17c7",
   "metadata": {},
   "source": [
    "Please replace the `CKPT_PATH` to where the checkpoints saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27518be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_PATH = '../configs/kl16_content12.yaml'\n",
    "CKPT_PATH = '../checkpoints/artfusion/artfusion_r12_step=317673.ckpt'\n",
    "\n",
    "H = 256\n",
    "W = 256\n",
    "DDIM_STEPS = 250\n",
    "ETA = 1.\n",
    "SEED = 2023\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65ef70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DualCondLDM: Running in eps-prediction mode\n",
      "DualConditionDiffusionWrapper has 181.02 M params.\n",
      "Keeping EMAs of 205.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 16, 16, 16) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Training DualCondLDM as an adaptive conditional model.\n",
      "Keeping EMAs of 3.\n",
      "Restored from ../checkpoints/artfusion/artfusion_r12_step=317673.ckpt with 0 missing and 0 unexpected keys\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 418.75 MiB already allocated; 6.56 MiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 26\u001B[0m\n\u001B[1;32m     24\u001B[0m config\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mfirst_stage_config\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     25\u001B[0m model \u001B[38;5;241m=\u001B[39m instantiate_from_config(config\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[0;32m---> 26\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:121\u001B[0m, in \u001B[0;36mDeviceDtypeModuleMixin.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39m_parse_to(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__update_properties(device\u001B[38;5;241m=\u001B[39mout[\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mout[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:989\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 989\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 641 (4 times)]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:664\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 664\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    665\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/anaconda3/envs/artfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:987\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m--> 987\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 418.75 MiB already allocated; 6.56 MiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "import albumentations\n",
    "\n",
    "from main import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "config = OmegaConf.load(CFG_PATH)\n",
    "config.model.params.ckpt_path = CKPT_PATH\n",
    "config.model.params.first_stage_config.params.ckpt_path = None\n",
    "model = instantiate_from_config(config.model)\n",
    "model = model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size=(W, H)):\n",
    "    image = Image.open(image_path)\n",
    "    if not image.mode == \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    image = image.resize(size)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    image = (image/127.5 - 1.0).astype(np.float32)\n",
    "    image = rearrange(image, 'h w c -> c h w')\n",
    "    return torch.from_numpy(image)\n",
    "\n",
    "\n",
    "def display_samples(samples, n_columns=1, figsize=(12, 12)):\n",
    "    if isinstance(samples, (list, tuple)):\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "        \n",
    "    samples = rearrange(samples, '(n m) c h w -> (m h) (n w) c', n=n_columns).cpu().numpy() * 255.\n",
    "    samples = Image.fromarray(samples.astype(np.uint8))\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "    plt.imshow(samples)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_rgb(x):\n",
    "    return torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2d5233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b48e4d84",
   "metadata": {},
   "source": [
    "## Style Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41baa8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 418.75 MiB already allocated; 6.56 MiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m style_image_paths \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/styles/d523d66a2f745aff1d3db21be993093fc.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/styles/the_scream.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/styles/Claude_Monet_73.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/styles/430f12a69a198bf3228f8177ed436624c.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m ]\n\u001B[0;32m----> 7\u001B[0m style_images \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpreprocess_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstyle_image_paths\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 418.75 MiB already allocated; 6.56 MiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "style_image_paths = [\n",
    "    '../data/styles/d523d66a2f745aff1d3db21be993093fc.jpg',\n",
    "    '../data/styles/the_scream.jpg',\n",
    "    '../data/styles/Claude_Monet_73.jpg',\n",
    "    '../data/styles/430f12a69a198bf3228f8177ed436624c.jpg'\n",
    "]\n",
    "style_images = torch.stack([preprocess_image(p) for p in style_image_paths], dim=0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfbc53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'style_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m display_samples(tensor_to_rgb(\u001B[43mstyle_images\u001B[49m), n_columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(style_images))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'style_images' is not defined"
     ]
    }
   ],
   "source": [
    "display_samples(tensor_to_rgb(style_images), n_columns=len(style_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4e858",
   "metadata": {},
   "source": [
    "Use the partial style-conditional model to visualize the style learned by ArtFusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = len(style_images)\n",
    "\n",
    "with torch.no_grad(), model.ema_scope(\"Plotting\"):\n",
    "    c_content = torch.zeros((bs, model.channels, model.image_size, model.image_size)).to(DEVICE)\n",
    "\n",
    "    vgg_features = model.vgg(model.vgg_scaling_layer(style_images))\n",
    "    c_style = model.get_style_features(vgg_features)\n",
    "\n",
    "    c = {'c1': c_content, 'c2': c_style}\n",
    "\n",
    "    samples = model.sample_log(cond=c, batch_size=bs, ddim=True, ddim_steps=DDIM_STEPS, eta=1.)[0]\n",
    "\n",
    "    x_samples = model.decode_first_stage(samples)\n",
    "    x_samples = tensor_to_rgb(x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7042c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_samples(x_samples, n_columns=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777c10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac02acd",
   "metadata": {},
   "source": [
    "## Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_style_features(content_image_path, style_image_path, h=H, w=W):\n",
    "    style_image = preprocess_image(style_image_path)[None, :].to(DEVICE)\n",
    "    content_image = preprocess_image(content_image_path, size=(w, h))[None, :].to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad(), model.ema_scope(\"Plotting\"):\n",
    "        vgg_features = model.vgg(model.vgg_scaling_layer(style_image))\n",
    "        c_style = model.get_style_features(vgg_features)\n",
    "        null_style = c_style.clone()\n",
    "        null_style[:] = model.null_style_vector.weight[0]\n",
    "\n",
    "        content_encoder_posterior = model.encode_first_stage(content_image)\n",
    "        content_encoder_posterior = model.get_first_stage_encoding(content_encoder_posterior)\n",
    "        c_content = model.get_content_features(content_encoder_posterior)\n",
    "        null_content = torch.zeros_like(c_content)\n",
    "        \n",
    "    c = {'c1': c_content, 'c2': c_style}\n",
    "    c_null_style = {'c1': c_content, 'c2': null_style}\n",
    "    c_null_content = {'c1': null_content, 'c2': c_style}\n",
    "    \n",
    "    return c, c_null_style, c_null_content\n",
    "    \n",
    "\n",
    "def style_transfer(\n",
    "    content_image_path, style_image_path,\n",
    "    h=H, w=W,\n",
    "    content_s=1., style_s=1.,\n",
    "    ddim_steps=DDIM_STEPS, eta=ETA,\n",
    "):\n",
    "    c, c_null_style, c_null_content = get_content_style_features(content_image_path, style_image_path, h, w)\n",
    "    \n",
    "    with torch.no_grad(), model.ema_scope(\"Plotting\"):\n",
    "        samples = model.sample_log(\n",
    "            cond=c, batch_size=1, x_T = torch.rand_like(c['c1']),\n",
    "            ddim=True, ddim_steps=ddim_steps, eta=eta,\n",
    "            unconditional_guidance_scale=content_s, unconditional_conditioning=c_null_content,\n",
    "            unconditional_guidance_scale_2=style_s, unconditional_conditioning_2=c_null_style)[0]\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = tensor_to_rgb(x_samples)\n",
    "    \n",
    "    return x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = '../data/contents/lofoton.jpg'\n",
    "style_image_path = '../data/styles/d523d66a2f745aff1d3db21be993093fc.jpg'\n",
    "\n",
    "style_image = preprocess_image(style_image_path)[None, :]\n",
    "content_image = preprocess_image(content_image_path)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples((tensor_to_rgb(content_image), tensor_to_rgb(style_image)), figsize=(6, 3), n_columns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8571666",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = style_transfer(content_image_path, style_image_path, content_s=0.5, style_s=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac60cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(x_samples, figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885ea67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2189ac",
   "metadata": {},
   "source": [
    "## Two Dimensional Classifier-Free Guidance (2D-CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08b3f9",
   "metadata": {},
   "source": [
    "The `content_scalers` and `style_scalers` are used to adjust the balance of content and style. With a higher content scaler, outputs will show clearer content. On the other hand, a higher style scaler results in a stronger stylization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79802840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_dim_cfg(\n",
    "    content_image_path, style_image_path,\n",
    "    content_scalers = [0.25, 0.5, 1.0, 2.0, 4.0], style_scalers = [0.15, 0.5, 1., 3., 5.],\n",
    "    ddim_steps=DDIM_STEPS, eta=ETA,\n",
    "):\n",
    "    c, c_null_style, c_null_content = get_content_style_features(content_image_path, style_image_path)\n",
    "    \n",
    "    with torch.no_grad(), model.ema_scope(\"Plotting\"):\n",
    "        samples = list()\n",
    "        for style_s in style_scalers:\n",
    "            for content_s in content_scalers:\n",
    "                sample = model.sample_log(\n",
    "                    cond=c, batch_size=1,\n",
    "                    ddim=True, ddim_steps=ddim_steps, eta=eta,\n",
    "                    unconditional_guidance_scale=content_s, unconditional_conditioning=c_null_content,\n",
    "                    unconditional_guidance_scale_2=style_s, unconditional_conditioning_2=c_null_style)[0]\n",
    "                samples.append(sample)\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = tensor_to_rgb(x_samples)\n",
    "        \n",
    "    return x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aca278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_samples = two_dim_cfg(content_image_path, style_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e446f",
   "metadata": {},
   "source": [
    "The content scaler increase from top to down, and style scalers increment from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e67d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_samples(x_samples, n_columns=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0004d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ba0b1fe",
   "metadata": {},
   "source": [
    "## Quick Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb43c98",
   "metadata": {},
   "source": [
    "For quick sampling, we can use only 10 `ddim_steps` along with `eta = 0.0` without serious loss of visual quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8956d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_samples = two_dim_cfg(content_image_path, style_image_path, ddim_steps=10, eta=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(x_samples, n_columns=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342da2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5677ace0",
   "metadata": {},
   "source": [
    "## High Resolution Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036ac1b",
   "metadata": {},
   "source": [
    "We can adopt lower `content_s` when working on high-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = '../data/contents/sicao.jpg'\n",
    "style_image_path = '../data/styles/df66247b2563f34f74259b4946d3c22ac.jpg'\n",
    "\n",
    "style_image = preprocess_image(style_image_path)[None, :]\n",
    "content_image = preprocess_image(content_image_path, size=(1920, 480))[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a743262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_samples(tensor_to_rgb(content_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61569c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(tensor_to_rgb(style_image), figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c22657",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = style_transfer(content_image_path, style_image_path, h=480, w=1920, content_s=0.3, style_s=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cccea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
